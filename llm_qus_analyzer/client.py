import time
from copy import deepcopy
from dataclasses import dataclass
from .settings import Settings
from langchain_together import ChatTogether
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from langchain_openai import ChatOpenAI
from langchain_deepseek import ChatDeepSeek

_model_inst={
    "together":ChatTogether,
    "chatgpt":ChatOpenAI,
    "deepseek":ChatDeepSeek
}
@dataclass
class LLMUsage:
    duration: float
    """The time taken for the interaction in seconds."""

    num_token_in: int
    """The number of input tokens used in the LLM interaction."""

    num_token_out: int
    """The number of output tokens generated by the LLM."""

    @classmethod
    def from_metadata(cls, metadata: dict, duration: float) -> "LLMUsage":
        """Creates an LLMUsage instance from LLM metadata and duration.

        Args:
            metadata (dict): Dictionary containing token usage information with keys:
                     - 'input_tokens': Count of input tokens
                     - 'output_tokens': Count of output tokens
            duration (float): Measured duration of the interaction in seconds

        Returns:
            LLMUsage: A new instance populated with the usage metrics

        Example:
            >>> metadata = {'input_tokens': 150, 'output_tokens': 75}
            >>> usage = LLMUsage.from_metadata(metadata, 2.5)
            >>> usage.num_token_in
            150
        """
        return LLMUsage(
            duration=duration,
            num_token_in=metadata["input_tokens"],
            num_token_out=metadata["output_tokens"],
        )


@dataclass
class LLMResult:
    """Stores the results of an LLM interaction including content and performance metrics."""

    content: str
    """The textual output generated by the LLM."""

    usage: LLMUsage
    """Object containing LLM usage statistics."""

    @classmethod
    def from_message(cls, message: AIMessage, duration: float):
        """Creates an LLMResult from an AIMessage and duration.

        Args:
            message (AIMessage): The message object returned by the LLM.
            duration (float): The time taken for the interaction in seconds.

        Returns:
            LLMResult: A new LLMResult instance populated from the message and duration.
        """
        usage = LLMUsage.from_metadata(
            metadata=message.usage_metadata, duration=duration
        )
        return LLMResult(content=message.content, usage=usage)

class LLMClient:
    """Client for interacting with multiple LLM models with prompt management."""

    def __init__(self, from_settings: Settings | None = None):
        """Initializes the LLMClient with models from settings.

        Args:
            from_settings (Settings | None): Optional custom settings to use instead of default.
                If None, uses the default settings.
        """
        used_config = deepcopy(from_settings.config) if from_settings else Settings()
        self.__models = [
            _model_inst[model.source](
                model=model.id,
                temperature=0,
                max_tokens=None,
                seed=42
            )
            for model in used_config.llm_models
            if model.source in _model_inst
        ]
        self.names = [model.name for model in used_config.llm_models]
        """Names of the available LLM models."""

        self.n_models = len(self.__models)
        """Number of available LLM models."""

        self.__prompt_key = None

    def inject_prompt(self, key: str, prompt: ChatPromptTemplate) -> None:
        """Injects a prompt template to be used with the LLM models.

        Args:
            key (str): Unique identifier for the prompt to avoid unnecessary re-initialization.
            prompt (ChatPromptTemplate): The prompt template to use with the LLM models.

        Note:
            If the same key is provided multiple times, the prompt will only be set once.
        """
        if self.__prompt_key == key:
            return
        self.__prompt_key = key
        self.__chains = [prompt | model for model in self.__models]

    def run(self, values: dict, model_idxs: list[int] = []) -> dict[int, LLMResult]:
        """Executes the LLM interaction with the provided input values.

        Args:
            values (dict): Input values to be used with the injected prompt template.
            model_idxs (list[int]): Optional list of model indices to use. If empty,
                all available models will be used.

        Returns:
            dict[int,LLMResult]: Dictionary mapping model indices to their results.

        Raises:
            ValueError: If any model index in model_idxs is out of bounds.
            NotImplementedError: If no prompt has been injected before running.
        """
        n = len(model_idxs)
        used_model_idxs = model_idxs[:] if n > 0 else list(range(self.n_models))
        for i in used_model_idxs:
            if i < 0 or i >= self.n_models:
                raise ValueError(
                    f"Invalid model index: {i}. Must be between 0 and {len(self.n_models) - 1}."
                )
        if not hasattr(self, "_LLMClient__chains"):
            raise NotImplementedError(
                "There is no prompt injected yet. Please inject the prompt first"
            )
        results: dict[int, LLMResult] = {}
        for i in used_model_idxs:
            time.sleep(1)
            start_time = time.time()
            message = self.__chains[i].invoke(values)
            end_time = time.time()
            duration = end_time - start_time
            results[i] = LLMResult.from_message(message, duration)
        return results
